{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import Conv1D, MaxPool1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from python_speech_features import mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mfcc(config, df, path='clean/'):\n",
    "    'builds 64 mfcc features (64x9)'\n",
    "    c = config\n",
    "    X = []\n",
    "    y = []\n",
    "    _min, _max = float('inf'), float('-inf')\n",
    "    for f in tqdm(df.index):\n",
    "        rate, wav = wavfile.read(path+f)\n",
    "        wav = wav.sum(axis=1)\n",
    "        wav = wav.astype(float)\n",
    "        label = df.at[f, 'label']\n",
    "        step = c.step\n",
    "        for i in range(0, len(wav), step):\n",
    "            partition = i+step\n",
    "            if step > wav.shape[0]:\n",
    "                signal = np.zeros((step, 1))\n",
    "                signal[:wav.shape[0], :] = wav.reshape(-1, 1)\n",
    "                X_mfcc = mfcc(signal, rate,\n",
    "                                   numcep=c.nfeat, nfilt=c.nfeat, nfft=c.nfft).T\n",
    "            elif partition > len(wav):\n",
    "                X_mfcc = mfcc(wav[-step:], rate,\n",
    "                                   numcep=c.nfeat, nfilt=c.nfeat, nfft=c.nfft).T\n",
    "            else:\n",
    "                X_mfcc = mfcc(wav[i:i+step], rate,\n",
    "                                   numcep=c.nfeat, nfilt=c.nfeat, nfft=c.nfft).T\n",
    "            _min = min(np.amin(X_mfcc), _min)\n",
    "            _max = max(np.amax(X_mfcc), _max)\n",
    "            X.append(X_mfcc)\n",
    "            y.append(classes.index(label))\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X = (X - _min) / (_max - _min)\n",
    "    print(_min, _max)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    y = to_categorical(y, num_classes=2)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d(config):\n",
    "    'builds 1d data simply to show 1d convolutions are possible'\n",
    "    c = config\n",
    "    X = []\n",
    "    y = []\n",
    "    for f in df.index:\n",
    "        rate, wav = wavfile.read('clean/'+f)\n",
    "        label = df.at[f, 'label']\n",
    "        step = c.step\n",
    "        for i in range(0, len(wav), step):\n",
    "            partition = i+step\n",
    "            if step > wav.shape[0]:\n",
    "                signal = np.zeros((step, 1))\n",
    "                signal[:wav.shape[0], :] = wav\n",
    "            elif partition > len(wav):\n",
    "                signal = wav[-step:]\n",
    "            else:\n",
    "                signal = wav[i:i+step]\n",
    "            X.append(signal)\n",
    "            y.append(classes.index(label))\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    mms = MinMaxScaler()\n",
    "    X = mms.fit_transform(X)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    y = to_categorical(y, num_classes=10)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1d_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(16, 9, activation='relu', padding='same',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv1D(16, 9, activation='relu', padding='same'))\n",
    "    model.add(MaxPool1D(16))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Conv1D(32, 3, activation='relu', padding='same'))\n",
    "    model.add(Conv1D(32, 3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool1D(4))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "    model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
    "    model.add(MaxPool1D(4))\n",
    "    model.add(Dropout(rate=0.1))\n",
    "    model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
    "    model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu', strides=(2, 2),\n",
    "                     padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1),\n",
    "                     padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1),\n",
    "                     padding='same'))\n",
    "    model.add(MaxPool2D((2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, mode='two', nfeat=64, nfft=1103, rate=44100):\n",
    "        self.mode = mode\n",
    "        self.nfeat = nfeat\n",
    "        self.nfft = nfft\n",
    "        self.rate = rate\n",
    "        self.step = int(rate/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sounds.csv', header=None)\n",
    "df.rename(columns={0: \"fname\", 1: \"label\"}, inplace=True)\n",
    "df.set_index('fname', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in df.index:\n",
    "    rate, signal = wavfile.read('clean/'+f)\n",
    "    signal = signal.sum(axis=1)\n",
    "    df.at[f, 'length'] = signal.shape[0]/rate\n",
    "\n",
    "classes = list(np.unique(df.label))\n",
    "class_dist = df.groupby(['label'])['length'].mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Class Distribution', y=1.08)\n",
    "ax.pie(class_dist, labels=class_dist.index, autopct='%1.1f%%',\n",
    "       shadow=False, startangle=90)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(mode='two', nfeat=64, nfft=1103, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 242/242 [00:51<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-193.80485258273006 134.46716346352255\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 32, 5, 16)         416       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 5, 32)         4640      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 32, 5, 64)         18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                131136    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 154,818\n",
      "Trainable params: 154,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if config.mode == 'one':\n",
    "    X, y = build_1d(config)\n",
    "    y_flat = np.argmax(y, axis=1)\n",
    "    input_shape = (X.shape[1], 1)\n",
    "    model = get_1d_model()\n",
    "    \n",
    "elif config.mode == 'two':\n",
    "    X, y = build_mfcc(config, df)\n",
    "    y_flat = np.argmax(y, axis=1)\n",
    "    input_shape = (X.shape[1], X.shape[2], 1)\n",
    "    model = get_2d_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight('balanced',\n",
    "                                     np.unique(y_flat),\n",
    "                                     y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0804 12:13:54.442294  3412 deprecation.py:323] From D:\\Games\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16256/23070 [====================>.........] - ETA: 5:40 - loss: 0.6934 - acc: 0.531 - ETA: 1:37 - loss: 0.6966 - acc: 0.453 - ETA: 1:02 - loss: 0.6955 - acc: 0.477 - ETA: 48s - loss: 0.6948 - acc: 0.493 - ETA: 40s - loss: 0.6935 - acc: 0.51 - ETA: 37s - loss: 0.6928 - acc: 0.52 - ETA: 33s - loss: 0.6925 - acc: 0.51 - ETA: 31s - loss: 0.6906 - acc: 0.53 - ETA: 29s - loss: 0.6894 - acc: 0.53 - ETA: 28s - loss: 0.6884 - acc: 0.53 - ETA: 28s - loss: 0.6878 - acc: 0.53 - ETA: 27s - loss: 0.6871 - acc: 0.53 - ETA: 26s - loss: 0.6851 - acc: 0.53 - ETA: 26s - loss: 0.6836 - acc: 0.53 - ETA: 25s - loss: 0.6815 - acc: 0.54 - ETA: 25s - loss: 0.6780 - acc: 0.55 - ETA: 24s - loss: 0.6690 - acc: 0.57 - ETA: 23s - loss: 0.6614 - acc: 0.59 - ETA: 23s - loss: 0.6592 - acc: 0.59 - ETA: 23s - loss: 0.6498 - acc: 0.60 - ETA: 23s - loss: 0.6501 - acc: 0.60 - ETA: 22s - loss: 0.6365 - acc: 0.61 - ETA: 22s - loss: 0.6293 - acc: 0.62 - ETA: 22s - loss: 0.6197 - acc: 0.63 - ETA: 22s - loss: 0.6152 - acc: 0.64 - ETA: 21s - loss: 0.6089 - acc: 0.64 - ETA: 21s - loss: 0.6000 - acc: 0.65 - ETA: 21s - loss: 0.5913 - acc: 0.66 - ETA: 21s - loss: 0.5750 - acc: 0.67 - ETA: 20s - loss: 0.5663 - acc: 0.68 - ETA: 20s - loss: 0.5580 - acc: 0.68 - ETA: 20s - loss: 0.5461 - acc: 0.69 - ETA: 20s - loss: 0.5401 - acc: 0.69 - ETA: 20s - loss: 0.5338 - acc: 0.70 - ETA: 19s - loss: 0.5212 - acc: 0.71 - ETA: 19s - loss: 0.5140 - acc: 0.71 - ETA: 19s - loss: 0.5040 - acc: 0.72 - ETA: 19s - loss: 0.4924 - acc: 0.73 - ETA: 18s - loss: 0.4865 - acc: 0.73 - ETA: 18s - loss: 0.4760 - acc: 0.74 - ETA: 18s - loss: 0.4699 - acc: 0.74 - ETA: 18s - loss: 0.4639 - acc: 0.75 - ETA: 18s - loss: 0.4542 - acc: 0.75 - ETA: 17s - loss: 0.4430 - acc: 0.76 - ETA: 17s - loss: 0.4373 - acc: 0.76 - ETA: 17s - loss: 0.4310 - acc: 0.77 - ETA: 17s - loss: 0.4234 - acc: 0.77 - ETA: 17s - loss: 0.4174 - acc: 0.78 - ETA: 17s - loss: 0.4113 - acc: 0.78 - ETA: 17s - loss: 0.4077 - acc: 0.78 - ETA: 17s - loss: 0.4038 - acc: 0.78 - ETA: 17s - loss: 0.3986 - acc: 0.79 - ETA: 17s - loss: 0.3936 - acc: 0.79 - ETA: 16s - loss: 0.3893 - acc: 0.79 - ETA: 16s - loss: 0.3855 - acc: 0.79 - ETA: 16s - loss: 0.3794 - acc: 0.80 - ETA: 16s - loss: 0.3763 - acc: 0.80 - ETA: 16s - loss: 0.3720 - acc: 0.80 - ETA: 16s - loss: 0.3683 - acc: 0.81 - ETA: 16s - loss: 0.3616 - acc: 0.81 - ETA: 16s - loss: 0.3591 - acc: 0.81 - ETA: 16s - loss: 0.3559 - acc: 0.81 - ETA: 16s - loss: 0.3534 - acc: 0.81 - ETA: 16s - loss: 0.3506 - acc: 0.82 - ETA: 15s - loss: 0.3464 - acc: 0.82 - ETA: 15s - loss: 0.3427 - acc: 0.82 - ETA: 15s - loss: 0.3375 - acc: 0.82 - ETA: 15s - loss: 0.3335 - acc: 0.83 - ETA: 15s - loss: 0.3326 - acc: 0.83 - ETA: 15s - loss: 0.3282 - acc: 0.83 - ETA: 15s - loss: 0.3259 - acc: 0.83 - ETA: 15s - loss: 0.3223 - acc: 0.83 - ETA: 15s - loss: 0.3186 - acc: 0.84 - ETA: 15s - loss: 0.3139 - acc: 0.84 - ETA: 14s - loss: 0.3104 - acc: 0.84 - ETA: 14s - loss: 0.3068 - acc: 0.84 - ETA: 14s - loss: 0.3025 - acc: 0.84 - ETA: 14s - loss: 0.2996 - acc: 0.85 - ETA: 14s - loss: 0.2962 - acc: 0.85 - ETA: 14s - loss: 0.2933 - acc: 0.85 - ETA: 14s - loss: 0.2892 - acc: 0.85 - ETA: 14s - loss: 0.2859 - acc: 0.85 - ETA: 14s - loss: 0.2834 - acc: 0.85 - ETA: 14s - loss: 0.2795 - acc: 0.86 - ETA: 13s - loss: 0.2768 - acc: 0.86 - ETA: 13s - loss: 0.2747 - acc: 0.86 - ETA: 13s - loss: 0.2717 - acc: 0.86 - ETA: 13s - loss: 0.2695 - acc: 0.86 - ETA: 13s - loss: 0.2663 - acc: 0.86 - ETA: 13s - loss: 0.2639 - acc: 0.86 - ETA: 13s - loss: 0.2610 - acc: 0.87 - ETA: 13s - loss: 0.2583 - acc: 0.87 - ETA: 13s - loss: 0.2573 - acc: 0.87 - ETA: 13s - loss: 0.2542 - acc: 0.87 - ETA: 13s - loss: 0.2515 - acc: 0.87 - ETA: 12s - loss: 0.2495 - acc: 0.87 - ETA: 12s - loss: 0.2466 - acc: 0.87 - ETA: 12s - loss: 0.2446 - acc: 0.88 - ETA: 12s - loss: 0.2420 - acc: 0.88 - ETA: 12s - loss: 0.2402 - acc: 0.88 - ETA: 12s - loss: 0.2384 - acc: 0.88 - ETA: 12s - loss: 0.2360 - acc: 0.88 - ETA: 12s - loss: 0.2346 - acc: 0.88 - ETA: 12s - loss: 0.2329 - acc: 0.88 - ETA: 12s - loss: 0.2306 - acc: 0.88 - ETA: 12s - loss: 0.2281 - acc: 0.88 - ETA: 12s - loss: 0.2257 - acc: 0.89 - ETA: 12s - loss: 0.2241 - acc: 0.89 - ETA: 11s - loss: 0.2221 - acc: 0.89 - ETA: 11s - loss: 0.2207 - acc: 0.89 - ETA: 11s - loss: 0.2201 - acc: 0.89 - ETA: 11s - loss: 0.2189 - acc: 0.89 - ETA: 11s - loss: 0.2174 - acc: 0.89 - ETA: 11s - loss: 0.2158 - acc: 0.89 - ETA: 11s - loss: 0.2146 - acc: 0.89 - ETA: 11s - loss: 0.2131 - acc: 0.89 - ETA: 11s - loss: 0.2117 - acc: 0.89 - ETA: 11s - loss: 0.2105 - acc: 0.89 - ETA: 11s - loss: 0.2092 - acc: 0.89 - ETA: 11s - loss: 0.2079 - acc: 0.89 - ETA: 11s - loss: 0.2067 - acc: 0.89 - ETA: 11s - loss: 0.2054 - acc: 0.90 - ETA: 11s - loss: 0.2039 - acc: 0.90 - ETA: 11s - loss: 0.2024 - acc: 0.90 - ETA: 10s - loss: 0.2008 - acc: 0.90 - ETA: 10s - loss: 0.1995 - acc: 0.90 - ETA: 10s - loss: 0.1985 - acc: 0.90 - ETA: 10s - loss: 0.1969 - acc: 0.90 - ETA: 10s - loss: 0.1952 - acc: 0.90 - ETA: 10s - loss: 0.1941 - acc: 0.90 - ETA: 10s - loss: 0.1924 - acc: 0.90 - ETA: 10s - loss: 0.1912 - acc: 0.90 - ETA: 10s - loss: 0.1902 - acc: 0.90 - ETA: 10s - loss: 0.1890 - acc: 0.90 - ETA: 10s - loss: 0.1877 - acc: 0.90 - ETA: 10s - loss: 0.1868 - acc: 0.91 - ETA: 10s - loss: 0.1857 - acc: 0.91 - ETA: 10s - loss: 0.1847 - acc: 0.91 - ETA: 9s - loss: 0.1836 - acc: 0.9118 - ETA: 9s - loss: 0.1826 - acc: 0.912 - ETA: 9s - loss: 0.1816 - acc: 0.912 - ETA: 9s - loss: 0.1807 - acc: 0.913 - ETA: 9s - loss: 0.1798 - acc: 0.913 - ETA: 9s - loss: 0.1788 - acc: 0.914 - ETA: 9s - loss: 0.1774 - acc: 0.914 - ETA: 9s - loss: 0.1765 - acc: 0.915 - ETA: 9s - loss: 0.1756 - acc: 0.915 - ETA: 9s - loss: 0.1746 - acc: 0.916 - ETA: 9s - loss: 0.1732 - acc: 0.917 - ETA: 9s - loss: 0.1726 - acc: 0.917 - ETA: 9s - loss: 0.1719 - acc: 0.917 - ETA: 9s - loss: 0.1706 - acc: 0.918 - ETA: 9s - loss: 0.1699 - acc: 0.918 - ETA: 9s - loss: 0.1690 - acc: 0.919 - ETA: 9s - loss: 0.1678 - acc: 0.919 - ETA: 8s - loss: 0.1669 - acc: 0.920 - ETA: 8s - loss: 0.1661 - acc: 0.920 - ETA: 8s - loss: 0.1653 - acc: 0.921 - ETA: 8s - loss: 0.1642 - acc: 0.921 - ETA: 8s - loss: 0.1634 - acc: 0.922 - ETA: 8s - loss: 0.1622 - acc: 0.922 - ETA: 8s - loss: 0.1614 - acc: 0.923 - ETA: 8s - loss: 0.1610 - acc: 0.923 - ETA: 8s - loss: 0.1606 - acc: 0.923 - ETA: 8s - loss: 0.1598 - acc: 0.924 - ETA: 8s - loss: 0.1603 - acc: 0.924 - ETA: 8s - loss: 0.1597 - acc: 0.924 - ETA: 8s - loss: 0.1592 - acc: 0.925 - ETA: 8s - loss: 0.1582 - acc: 0.925 - ETA: 8s - loss: 0.1574 - acc: 0.925 - ETA: 8s - loss: 0.1571 - acc: 0.926 - ETA: 7s - loss: 0.1564 - acc: 0.926 - ETA: 7s - loss: 0.1556 - acc: 0.927 - ETA: 7s - loss: 0.1549 - acc: 0.927 - ETA: 7s - loss: 0.1542 - acc: 0.927 - ETA: 7s - loss: 0.1536 - acc: 0.927 - ETA: 7s - loss: 0.1529 - acc: 0.928 - ETA: 7s - loss: 0.1522 - acc: 0.928 - ETA: 7s - loss: 0.1513 - acc: 0.929 - ETA: 7s - loss: 0.1506 - acc: 0.929 - ETA: 7s - loss: 0.1500 - acc: 0.929 - ETA: 7s - loss: 0.1494 - acc: 0.930 - ETA: 7s - loss: 0.1488 - acc: 0.930 - ETA: 7s - loss: 0.1478 - acc: 0.930 - ETA: 7s - loss: 0.1472 - acc: 0.931 - ETA: 7s - loss: 0.1464 - acc: 0.931 - ETA: 7s - loss: 0.1457 - acc: 0.931 - ETA: 7s - loss: 0.1453 - acc: 0.932 - ETA: 6s - loss: 0.1449 - acc: 0.932 - ETA: 6s - loss: 0.1442 - acc: 0.932 - ETA: 6s - loss: 0.1435 - acc: 0.932 - ETA: 6s - loss: 0.1427 - acc: 0.933 - ETA: 6s - loss: 0.1426 - acc: 0.933 - ETA: 6s - loss: 0.1418 - acc: 0.933 - ETA: 6s - loss: 0.1418 - acc: 0.934 - ETA: 6s - loss: 0.1409 - acc: 0.934 - ETA: 6s - loss: 0.1402 - acc: 0.934 - ETA: 6s - loss: 0.1399 - acc: 0.934 - ETA: 6s - loss: 0.1394 - acc: 0.935 - ETA: 6s - loss: 0.1389 - acc: 0.935 - ETA: 6s - loss: 0.1383 - acc: 0.935 - ETA: 6s - loss: 0.1378 - acc: 0.935 - ETA: 6s - loss: 0.1372 - acc: 0.936 - ETA: 6s - loss: 0.1367 - acc: 0.936 - ETA: 5s - loss: 0.1361 - acc: 0.936 - ETA: 5s - loss: 0.1353 - acc: 0.937 - ETA: 5s - loss: 0.1348 - acc: 0.937 - ETA: 5s - loss: 0.1343 - acc: 0.937 - ETA: 5s - loss: 0.1338 - acc: 0.937 - ETA: 5s - loss: 0.1338 - acc: 0.938 - ETA: 5s - loss: 0.1336 - acc: 0.938 - ETA: 5s - loss: 0.1332 - acc: 0.938 - ETA: 5s - loss: 0.1328 - acc: 0.938 - ETA: 5s - loss: 0.1323 - acc: 0.9389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23070/23070 [==============================] - ETA: 5s - loss: 0.1318 - acc: 0.939 - ETA: 5s - loss: 0.1313 - acc: 0.939 - ETA: 5s - loss: 0.1305 - acc: 0.939 - ETA: 5s - loss: 0.1298 - acc: 0.940 - ETA: 5s - loss: 0.1296 - acc: 0.940 - ETA: 5s - loss: 0.1289 - acc: 0.940 - ETA: 5s - loss: 0.1285 - acc: 0.940 - ETA: 4s - loss: 0.1279 - acc: 0.941 - ETA: 4s - loss: 0.1273 - acc: 0.941 - ETA: 4s - loss: 0.1273 - acc: 0.941 - ETA: 4s - loss: 0.1268 - acc: 0.941 - ETA: 4s - loss: 0.1265 - acc: 0.942 - ETA: 4s - loss: 0.1259 - acc: 0.942 - ETA: 4s - loss: 0.1256 - acc: 0.942 - ETA: 4s - loss: 0.1254 - acc: 0.942 - ETA: 4s - loss: 0.1248 - acc: 0.942 - ETA: 4s - loss: 0.1243 - acc: 0.943 - ETA: 4s - loss: 0.1237 - acc: 0.943 - ETA: 4s - loss: 0.1237 - acc: 0.943 - ETA: 4s - loss: 0.1233 - acc: 0.943 - ETA: 3s - loss: 0.1229 - acc: 0.944 - ETA: 3s - loss: 0.1226 - acc: 0.944 - ETA: 3s - loss: 0.1224 - acc: 0.944 - ETA: 3s - loss: 0.1224 - acc: 0.944 - ETA: 3s - loss: 0.1221 - acc: 0.944 - ETA: 3s - loss: 0.1216 - acc: 0.945 - ETA: 3s - loss: 0.1210 - acc: 0.945 - ETA: 3s - loss: 0.1208 - acc: 0.945 - ETA: 3s - loss: 0.1204 - acc: 0.945 - ETA: 3s - loss: 0.1199 - acc: 0.945 - ETA: 3s - loss: 0.1195 - acc: 0.946 - ETA: 3s - loss: 0.1189 - acc: 0.946 - ETA: 3s - loss: 0.1184 - acc: 0.946 - ETA: 3s - loss: 0.1181 - acc: 0.946 - ETA: 2s - loss: 0.1175 - acc: 0.947 - ETA: 2s - loss: 0.1170 - acc: 0.947 - ETA: 2s - loss: 0.1164 - acc: 0.947 - ETA: 2s - loss: 0.1162 - acc: 0.947 - ETA: 2s - loss: 0.1159 - acc: 0.947 - ETA: 2s - loss: 0.1158 - acc: 0.948 - ETA: 2s - loss: 0.1154 - acc: 0.948 - ETA: 2s - loss: 0.1151 - acc: 0.948 - ETA: 2s - loss: 0.1146 - acc: 0.948 - ETA: 2s - loss: 0.1140 - acc: 0.949 - ETA: 2s - loss: 0.1141 - acc: 0.949 - ETA: 2s - loss: 0.1140 - acc: 0.949 - ETA: 2s - loss: 0.1135 - acc: 0.949 - ETA: 1s - loss: 0.1131 - acc: 0.949 - ETA: 1s - loss: 0.1129 - acc: 0.949 - ETA: 1s - loss: 0.1125 - acc: 0.949 - ETA: 1s - loss: 0.1122 - acc: 0.950 - ETA: 1s - loss: 0.1118 - acc: 0.950 - ETA: 1s - loss: 0.1115 - acc: 0.950 - ETA: 1s - loss: 0.1112 - acc: 0.950 - ETA: 1s - loss: 0.1107 - acc: 0.950 - ETA: 1s - loss: 0.1105 - acc: 0.950 - ETA: 1s - loss: 0.1100 - acc: 0.951 - ETA: 1s - loss: 0.1098 - acc: 0.951 - ETA: 1s - loss: 0.1095 - acc: 0.951 - ETA: 1s - loss: 0.1092 - acc: 0.951 - ETA: 1s - loss: 0.1089 - acc: 0.951 - ETA: 1s - loss: 0.1089 - acc: 0.951 - ETA: 1s - loss: 0.1086 - acc: 0.951 - ETA: 1s - loss: 0.1082 - acc: 0.952 - ETA: 0s - loss: 0.1079 - acc: 0.952 - ETA: 0s - loss: 0.1075 - acc: 0.952 - ETA: 0s - loss: 0.1072 - acc: 0.952 - ETA: 0s - loss: 0.1070 - acc: 0.952 - ETA: 0s - loss: 0.1067 - acc: 0.952 - ETA: 0s - loss: 0.1064 - acc: 0.952 - ETA: 0s - loss: 0.1059 - acc: 0.953 - ETA: 0s - loss: 0.1056 - acc: 0.953 - ETA: 0s - loss: 0.1054 - acc: 0.953 - ETA: 0s - loss: 0.1051 - acc: 0.953 - ETA: 0s - loss: 0.1046 - acc: 0.953 - ETA: 0s - loss: 0.1045 - acc: 0.953 - ETA: 0s - loss: 0.1043 - acc: 0.953 - ETA: 0s - loss: 0.1040 - acc: 0.954 - ETA: 0s - loss: 0.1037 - acc: 0.954 - ETA: 0s - loss: 0.1033 - acc: 0.954 - ETA: 0s - loss: 0.1030 - acc: 0.954 - 18s 791us/step - loss: 0.1028 - acc: 0.9545\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15712/23070 [===================>..........] - ETA: 18s - loss: 0.0750 - acc: 0.96 - ETA: 18s - loss: 0.0649 - acc: 0.97 - ETA: 17s - loss: 0.0336 - acc: 0.98 - ETA: 18s - loss: 0.0257 - acc: 0.99 - ETA: 18s - loss: 0.0214 - acc: 0.99 - ETA: 18s - loss: 0.0301 - acc: 0.99 - ETA: 18s - loss: 0.0265 - acc: 0.99 - ETA: 18s - loss: 0.0242 - acc: 0.99 - ETA: 17s - loss: 0.0216 - acc: 0.99 - ETA: 17s - loss: 0.0253 - acc: 0.99 - ETA: 17s - loss: 0.0234 - acc: 0.99 - ETA: 17s - loss: 0.0262 - acc: 0.99 - ETA: 17s - loss: 0.0252 - acc: 0.99 - ETA: 17s - loss: 0.0269 - acc: 0.99 - ETA: 17s - loss: 0.0286 - acc: 0.99 - ETA: 17s - loss: 0.0338 - acc: 0.99 - ETA: 17s - loss: 0.0318 - acc: 0.99 - ETA: 17s - loss: 0.0306 - acc: 0.99 - ETA: 17s - loss: 0.0328 - acc: 0.99 - ETA: 17s - loss: 0.0314 - acc: 0.99 - ETA: 17s - loss: 0.0303 - acc: 0.99 - ETA: 17s - loss: 0.0292 - acc: 0.99 - ETA: 17s - loss: 0.0308 - acc: 0.99 - ETA: 17s - loss: 0.0299 - acc: 0.99 - ETA: 17s - loss: 0.0285 - acc: 0.99 - ETA: 17s - loss: 0.0276 - acc: 0.99 - ETA: 17s - loss: 0.0269 - acc: 0.99 - ETA: 17s - loss: 0.0288 - acc: 0.99 - ETA: 17s - loss: 0.0280 - acc: 0.99 - ETA: 17s - loss: 0.0271 - acc: 0.99 - ETA: 17s - loss: 0.0264 - acc: 0.99 - ETA: 17s - loss: 0.0255 - acc: 0.99 - ETA: 17s - loss: 0.0249 - acc: 0.99 - ETA: 17s - loss: 0.0259 - acc: 0.99 - ETA: 16s - loss: 0.0254 - acc: 0.99 - ETA: 16s - loss: 0.0251 - acc: 0.99 - ETA: 16s - loss: 0.0246 - acc: 0.99 - ETA: 16s - loss: 0.0242 - acc: 0.99 - ETA: 16s - loss: 0.0238 - acc: 0.99 - ETA: 16s - loss: 0.0233 - acc: 0.99 - ETA: 16s - loss: 0.0227 - acc: 0.99 - ETA: 16s - loss: 0.0223 - acc: 0.99 - ETA: 16s - loss: 0.0219 - acc: 0.99 - ETA: 16s - loss: 0.0215 - acc: 0.99 - ETA: 16s - loss: 0.0210 - acc: 0.99 - ETA: 16s - loss: 0.0204 - acc: 0.99 - ETA: 16s - loss: 0.0200 - acc: 0.99 - ETA: 16s - loss: 0.0198 - acc: 0.99 - ETA: 15s - loss: 0.0195 - acc: 0.99 - ETA: 15s - loss: 0.0192 - acc: 0.99 - ETA: 15s - loss: 0.0189 - acc: 0.99 - ETA: 15s - loss: 0.0186 - acc: 0.99 - ETA: 15s - loss: 0.0184 - acc: 0.99 - ETA: 15s - loss: 0.0184 - acc: 0.99 - ETA: 15s - loss: 0.0180 - acc: 0.99 - ETA: 15s - loss: 0.0177 - acc: 0.99 - ETA: 15s - loss: 0.0175 - acc: 0.99 - ETA: 15s - loss: 0.0172 - acc: 0.99 - ETA: 15s - loss: 0.0168 - acc: 0.99 - ETA: 15s - loss: 0.0166 - acc: 0.99 - ETA: 15s - loss: 0.0164 - acc: 0.99 - ETA: 15s - loss: 0.0162 - acc: 0.99 - ETA: 15s - loss: 0.0160 - acc: 0.99 - ETA: 15s - loss: 0.0161 - acc: 0.99 - ETA: 15s - loss: 0.0160 - acc: 0.99 - ETA: 15s - loss: 0.0159 - acc: 0.99 - ETA: 15s - loss: 0.0157 - acc: 0.99 - ETA: 14s - loss: 0.0155 - acc: 0.99 - ETA: 14s - loss: 0.0164 - acc: 0.99 - ETA: 14s - loss: 0.0162 - acc: 0.99 - ETA: 14s - loss: 0.0161 - acc: 0.99 - ETA: 14s - loss: 0.0159 - acc: 0.99 - ETA: 14s - loss: 0.0156 - acc: 0.99 - ETA: 14s - loss: 0.0154 - acc: 0.99 - ETA: 14s - loss: 0.0153 - acc: 0.99 - ETA: 14s - loss: 0.0151 - acc: 0.99 - ETA: 14s - loss: 0.0154 - acc: 0.99 - ETA: 14s - loss: 0.0168 - acc: 0.99 - ETA: 14s - loss: 0.0167 - acc: 0.99 - ETA: 14s - loss: 0.0168 - acc: 0.99 - ETA: 13s - loss: 0.0166 - acc: 0.99 - ETA: 13s - loss: 0.0163 - acc: 0.99 - ETA: 13s - loss: 0.0162 - acc: 0.99 - ETA: 13s - loss: 0.0160 - acc: 0.99 - ETA: 13s - loss: 0.0159 - acc: 0.99 - ETA: 13s - loss: 0.0159 - acc: 0.99 - ETA: 13s - loss: 0.0158 - acc: 0.99 - ETA: 13s - loss: 0.0157 - acc: 0.99 - ETA: 13s - loss: 0.0155 - acc: 0.99 - ETA: 13s - loss: 0.0159 - acc: 0.99 - ETA: 13s - loss: 0.0158 - acc: 0.99 - ETA: 13s - loss: 0.0156 - acc: 0.99 - ETA: 13s - loss: 0.0154 - acc: 0.99 - ETA: 13s - loss: 0.0153 - acc: 0.99 - ETA: 13s - loss: 0.0152 - acc: 0.99 - ETA: 12s - loss: 0.0150 - acc: 0.99 - ETA: 12s - loss: 0.0155 - acc: 0.99 - ETA: 12s - loss: 0.0153 - acc: 0.99 - ETA: 12s - loss: 0.0153 - acc: 0.99 - ETA: 12s - loss: 0.0152 - acc: 0.99 - ETA: 12s - loss: 0.0153 - acc: 0.99 - ETA: 12s - loss: 0.0153 - acc: 0.99 - ETA: 12s - loss: 0.0152 - acc: 0.99 - ETA: 12s - loss: 0.0156 - acc: 0.99 - ETA: 12s - loss: 0.0156 - acc: 0.99 - ETA: 12s - loss: 0.0155 - acc: 0.99 - ETA: 12s - loss: 0.0154 - acc: 0.99 - ETA: 12s - loss: 0.0152 - acc: 0.99 - ETA: 12s - loss: 0.0158 - acc: 0.99 - ETA: 12s - loss: 0.0156 - acc: 0.99 - ETA: 11s - loss: 0.0157 - acc: 0.99 - ETA: 11s - loss: 0.0157 - acc: 0.99 - ETA: 11s - loss: 0.0161 - acc: 0.99 - ETA: 11s - loss: 0.0160 - acc: 0.99 - ETA: 11s - loss: 0.0161 - acc: 0.99 - ETA: 11s - loss: 0.0160 - acc: 0.99 - ETA: 11s - loss: 0.0159 - acc: 0.99 - ETA: 11s - loss: 0.0159 - acc: 0.99 - ETA: 11s - loss: 0.0159 - acc: 0.99 - ETA: 11s - loss: 0.0159 - acc: 0.99 - ETA: 11s - loss: 0.0158 - acc: 0.99 - ETA: 11s - loss: 0.0163 - acc: 0.99 - ETA: 11s - loss: 0.0161 - acc: 0.99 - ETA: 11s - loss: 0.0160 - acc: 0.99 - ETA: 11s - loss: 0.0163 - acc: 0.99 - ETA: 11s - loss: 0.0164 - acc: 0.99 - ETA: 11s - loss: 0.0163 - acc: 0.99 - ETA: 11s - loss: 0.0164 - acc: 0.99 - ETA: 10s - loss: 0.0163 - acc: 0.99 - ETA: 10s - loss: 0.0163 - acc: 0.99 - ETA: 10s - loss: 0.0162 - acc: 0.99 - ETA: 10s - loss: 0.0161 - acc: 0.99 - ETA: 10s - loss: 0.0164 - acc: 0.99 - ETA: 10s - loss: 0.0163 - acc: 0.99 - ETA: 10s - loss: 0.0162 - acc: 0.99 - ETA: 10s - loss: 0.0161 - acc: 0.99 - ETA: 10s - loss: 0.0161 - acc: 0.99 - ETA: 10s - loss: 0.0159 - acc: 0.99 - ETA: 10s - loss: 0.0158 - acc: 0.99 - ETA: 10s - loss: 0.0157 - acc: 0.99 - ETA: 10s - loss: 0.0156 - acc: 0.99 - ETA: 10s - loss: 0.0154 - acc: 0.99 - ETA: 10s - loss: 0.0154 - acc: 0.99 - ETA: 10s - loss: 0.0158 - acc: 0.99 - ETA: 10s - loss: 0.0157 - acc: 0.99 - ETA: 9s - loss: 0.0156 - acc: 0.9961 - ETA: 9s - loss: 0.0156 - acc: 0.996 - ETA: 9s - loss: 0.0157 - acc: 0.996 - ETA: 9s - loss: 0.0158 - acc: 0.996 - ETA: 9s - loss: 0.0159 - acc: 0.996 - ETA: 9s - loss: 0.0158 - acc: 0.996 - ETA: 9s - loss: 0.0157 - acc: 0.996 - ETA: 9s - loss: 0.0161 - acc: 0.996 - ETA: 9s - loss: 0.0160 - acc: 0.996 - ETA: 9s - loss: 0.0159 - acc: 0.996 - ETA: 9s - loss: 0.0158 - acc: 0.996 - ETA: 9s - loss: 0.0158 - acc: 0.996 - ETA: 9s - loss: 0.0157 - acc: 0.996 - ETA: 9s - loss: 0.0157 - acc: 0.996 - ETA: 9s - loss: 0.0157 - acc: 0.996 - ETA: 9s - loss: 0.0159 - acc: 0.996 - ETA: 9s - loss: 0.0158 - acc: 0.996 - ETA: 9s - loss: 0.0158 - acc: 0.996 - ETA: 8s - loss: 0.0157 - acc: 0.996 - ETA: 8s - loss: 0.0157 - acc: 0.996 - ETA: 8s - loss: 0.0156 - acc: 0.996 - ETA: 8s - loss: 0.0155 - acc: 0.996 - ETA: 8s - loss: 0.0155 - acc: 0.996 - ETA: 8s - loss: 0.0154 - acc: 0.996 - ETA: 8s - loss: 0.0155 - acc: 0.996 - ETA: 8s - loss: 0.0154 - acc: 0.996 - ETA: 8s - loss: 0.0154 - acc: 0.996 - ETA: 8s - loss: 0.0153 - acc: 0.996 - ETA: 8s - loss: 0.0152 - acc: 0.996 - ETA: 8s - loss: 0.0151 - acc: 0.996 - ETA: 8s - loss: 0.0150 - acc: 0.996 - ETA: 8s - loss: 0.0150 - acc: 0.996 - ETA: 8s - loss: 0.0149 - acc: 0.996 - ETA: 8s - loss: 0.0149 - acc: 0.996 - ETA: 8s - loss: 0.0148 - acc: 0.996 - ETA: 7s - loss: 0.0148 - acc: 0.996 - ETA: 7s - loss: 0.0147 - acc: 0.996 - ETA: 7s - loss: 0.0147 - acc: 0.996 - ETA: 7s - loss: 0.0150 - acc: 0.996 - ETA: 7s - loss: 0.0150 - acc: 0.996 - ETA: 7s - loss: 0.0149 - acc: 0.996 - ETA: 7s - loss: 0.0151 - acc: 0.996 - ETA: 7s - loss: 0.0150 - acc: 0.996 - ETA: 7s - loss: 0.0150 - acc: 0.996 - ETA: 7s - loss: 0.0151 - acc: 0.996 - ETA: 7s - loss: 0.0150 - acc: 0.996 - ETA: 7s - loss: 0.0150 - acc: 0.996 - ETA: 7s - loss: 0.0149 - acc: 0.996 - ETA: 7s - loss: 0.0148 - acc: 0.996 - ETA: 7s - loss: 0.0153 - acc: 0.996 - ETA: 7s - loss: 0.0152 - acc: 0.996 - ETA: 7s - loss: 0.0156 - acc: 0.996 - ETA: 6s - loss: 0.0156 - acc: 0.996 - ETA: 6s - loss: 0.0155 - acc: 0.996 - ETA: 6s - loss: 0.0155 - acc: 0.996 - ETA: 6s - loss: 0.0154 - acc: 0.996 - ETA: 6s - loss: 0.0153 - acc: 0.996 - ETA: 6s - loss: 0.0152 - acc: 0.996 - ETA: 6s - loss: 0.0152 - acc: 0.996 - ETA: 6s - loss: 0.0157 - acc: 0.996 - ETA: 6s - loss: 0.0157 - acc: 0.996 - ETA: 6s - loss: 0.0157 - acc: 0.996 - ETA: 6s - loss: 0.0159 - acc: 0.996 - ETA: 6s - loss: 0.0159 - acc: 0.996 - ETA: 6s - loss: 0.0158 - acc: 0.996 - ETA: 6s - loss: 0.0159 - acc: 0.996 - ETA: 6s - loss: 0.0158 - acc: 0.996 - ETA: 6s - loss: 0.0158 - acc: 0.996 - ETA: 5s - loss: 0.0157 - acc: 0.996 - ETA: 5s - loss: 0.0158 - acc: 0.9961"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23070/23070 [==============================] - ETA: 5s - loss: 0.0157 - acc: 0.996 - ETA: 5s - loss: 0.0157 - acc: 0.996 - ETA: 5s - loss: 0.0157 - acc: 0.996 - ETA: 5s - loss: 0.0156 - acc: 0.996 - ETA: 5s - loss: 0.0156 - acc: 0.996 - ETA: 5s - loss: 0.0156 - acc: 0.996 - ETA: 5s - loss: 0.0156 - acc: 0.996 - ETA: 5s - loss: 0.0156 - acc: 0.996 - ETA: 5s - loss: 0.0155 - acc: 0.996 - ETA: 5s - loss: 0.0155 - acc: 0.996 - ETA: 5s - loss: 0.0155 - acc: 0.996 - ETA: 5s - loss: 0.0155 - acc: 0.996 - ETA: 4s - loss: 0.0154 - acc: 0.996 - ETA: 4s - loss: 0.0153 - acc: 0.996 - ETA: 4s - loss: 0.0152 - acc: 0.996 - ETA: 4s - loss: 0.0152 - acc: 0.996 - ETA: 4s - loss: 0.0151 - acc: 0.996 - ETA: 4s - loss: 0.0150 - acc: 0.996 - ETA: 4s - loss: 0.0150 - acc: 0.996 - ETA: 4s - loss: 0.0149 - acc: 0.996 - ETA: 4s - loss: 0.0148 - acc: 0.996 - ETA: 4s - loss: 0.0148 - acc: 0.996 - ETA: 4s - loss: 0.0147 - acc: 0.996 - ETA: 4s - loss: 0.0147 - acc: 0.996 - ETA: 4s - loss: 0.0146 - acc: 0.996 - ETA: 4s - loss: 0.0148 - acc: 0.996 - ETA: 3s - loss: 0.0148 - acc: 0.996 - ETA: 3s - loss: 0.0148 - acc: 0.996 - ETA: 3s - loss: 0.0147 - acc: 0.996 - ETA: 3s - loss: 0.0147 - acc: 0.996 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0145 - acc: 0.996 - ETA: 3s - loss: 0.0145 - acc: 0.996 - ETA: 3s - loss: 0.0144 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.996 - ETA: 3s - loss: 0.0142 - acc: 0.996 - ETA: 3s - loss: 0.0144 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.996 - ETA: 2s - loss: 0.0143 - acc: 0.996 - ETA: 2s - loss: 0.0143 - acc: 0.996 - ETA: 2s - loss: 0.0142 - acc: 0.996 - ETA: 2s - loss: 0.0142 - acc: 0.996 - ETA: 2s - loss: 0.0141 - acc: 0.996 - ETA: 2s - loss: 0.0141 - acc: 0.996 - ETA: 2s - loss: 0.0142 - acc: 0.996 - ETA: 2s - loss: 0.0141 - acc: 0.996 - ETA: 2s - loss: 0.0141 - acc: 0.996 - ETA: 2s - loss: 0.0141 - acc: 0.996 - ETA: 2s - loss: 0.0140 - acc: 0.996 - ETA: 2s - loss: 0.0139 - acc: 0.996 - ETA: 2s - loss: 0.0139 - acc: 0.996 - ETA: 2s - loss: 0.0139 - acc: 0.996 - ETA: 2s - loss: 0.0139 - acc: 0.996 - ETA: 2s - loss: 0.0138 - acc: 0.996 - ETA: 2s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0140 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0139 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0138 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 1s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0137 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0136 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0135 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0134 - acc: 0.996 - ETA: 0s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0133 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - ETA: 0s - loss: 0.0132 - acc: 0.996 - 18s 796us/step - loss: 0.0132 - acc: 0.9967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241802254a8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=2, batch_size=32,\n",
    "          shuffle=True,\n",
    "          class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yaml = model.to_yaml()\n",
    "with open('mfcc.yaml', 'w') as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "model.save_weights('mfcc.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7401915e-05, 2.6598203e-01],\n",
       "       [3.3646822e-05, 2.4973902e-01],\n",
       "       [1.7374754e-05, 3.7783152e-01],\n",
       "       ...,\n",
       "       [9.9976891e-01, 0.0000000e+00],\n",
       "       [9.9533075e-01, 0.0000000e+00],\n",
       "       [9.9991906e-01, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
